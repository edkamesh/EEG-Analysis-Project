{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyedflib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj8WOJpXgP6s",
        "outputId": "053c3257-42e1-4a1b-ad0a-9d5c7daab06e"
      },
      "id": "aj8WOJpXgP6s",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyedflib\n",
            "  Downloading pyEDFlib-0.1.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from pyedflib) (1.25.2)\n",
            "Installing collected packages: pyedflib\n",
            "Successfully installed pyedflib-0.1.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1c98468-3a1a-49bb-9d5b-4c05851aaf3a",
      "metadata": {
        "id": "c1c98468-3a1a-49bb-9d5b-4c05851aaf3a"
      },
      "source": [
        "## EEG Data Preprocessing and EEGNet Model Training\n",
        "### Introduction\n",
        "\n",
        "This documentation provides a step-by-step guide on how to preprocess EEG data from EDF files, segment the data, and train an EEGNet model using TensorFlow/Keras. The dataset used is from the PhysioNet EEG recordings of subjects before and during mental arithmetic tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e380368c-4ae5-42de-bcc1-76a7c671a901",
      "metadata": {
        "id": "e380368c-4ae5-42de-bcc1-76a7c671a901"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pyedflib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.signal import resample\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Uspwzlwd_c8",
        "outputId": "e76d67e2-2bc5-414f-c622-cf6618ac665b"
      },
      "id": "2Uspwzlwd_c8",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d03ec33c-a400-46f1-97f0-693f382c4a3d",
      "metadata": {
        "id": "d03ec33c-a400-46f1-97f0-693f382c4a3d"
      },
      "source": [
        "## Step 1: Loading EDF Files and Extracting Signals\n",
        "### Objective: Load EEG data from EDF files and extract the signals into numpy arrays.\n",
        "\n",
        "Explanation: EDF (European Data Format) files are commonly used for storing EEG data. The MNE library in Python is a powerful tool for loading and manipulating EEG data from EDF files. In this step, we read the EDF files from the specified folder, extract the EEG signals, and store them in a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "41030143-e355-47bd-bcb3-ee174e44f5a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41030143-e355-47bd-bcb3-ee174e44f5a4",
        "outputId": "6aa72e8d-520f-4120-92cb-c0a8157ffbb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_raw shape: (72, 21, 94000)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load and Extract Signals from EDF Files\n",
        "def load_edf_files(folder_path):\n",
        "    edf_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.edf')]\n",
        "    all_signals = []\n",
        "    max_samples = 0\n",
        "\n",
        "    # Determine the maximum number of samples across all files\n",
        "    for edf_file in edf_files:\n",
        "        edf_reader = pyedflib.EdfReader(edf_file)\n",
        "        max_samples = max(max_samples, edf_reader.getNSamples()[0])\n",
        "        edf_reader.close()\n",
        "\n",
        "    for edf_file in edf_files:\n",
        "        edf_reader = pyedflib.EdfReader(edf_file)\n",
        "        num_channels = edf_reader.signals_in_file\n",
        "        signals = np.zeros((num_channels, max_samples))\n",
        "\n",
        "        for i in range(num_channels):\n",
        "            signal = edf_reader.readSignal(i)\n",
        "            if len(signal) < max_samples:\n",
        "                # Pad with zeros if the signal is shorter than the max_samples\n",
        "                signal = np.pad(signal, (0, max_samples - len(signal)), 'constant')\n",
        "            signals[i, :] = signal\n",
        "\n",
        "        all_signals.append(signals)\n",
        "        edf_reader.close()\n",
        "\n",
        "    return np.array(all_signals), edf_files\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/eeg-during-mental-arithmetic-tasks-1.0.0/\"\n",
        "X_raw, edf_files = load_edf_files(folder_path)\n",
        "print(f\"X_raw shape: {X_raw.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "652b03d4-2a64-41da-8be9-903c06cfdd8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "652b03d4-2a64-41da-8be9-903c06cfdd8e",
        "outputId": "786132b6-af11-4e12-c856-911fb3b0f90d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_raw shape: (72,)\n",
            "y_raw: [1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 0]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load subject information\n",
        "subject_info = pd.read_csv(\"/content/subject-info.csv\")\n",
        "\n",
        "# Extract labels (assuming 'Count quality' is the column for labels)\n",
        "labels_dict = {str(row['Subject']): row['Count quality'] for _, row in subject_info.iterrows()}\n",
        "\n",
        "# Extract labels from file names\n",
        "def get_labels(edf_files, labels_dict):\n",
        "    labels = []\n",
        "    for file in edf_files:\n",
        "        subject_id = os.path.basename(file).split('_')[0]\n",
        "        if subject_id in labels_dict:\n",
        "            labels.append(labels_dict[subject_id])\n",
        "        else:\n",
        "            labels.append(None)  # Handle missing labels if necessary\n",
        "    return np.array(labels)\n",
        "\n",
        "y_raw = get_labels(edf_files, labels_dict)\n",
        "print(f\"y_raw shape: {y_raw.shape}\")\n",
        "print(f\"y_raw: {y_raw}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b94add22-23bd-4694-b405-418116b50acb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b94add22-23bd-4694-b405-418116b50acb",
        "outputId": "1129fbf3-cde5-4ead-dbd4-c95cdd9d0e1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_scaled shape: (72, 94000, 21, 1)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def preprocess_signals(signals):\n",
        "    # Transpose to have shape (samples, channels, time_points)\n",
        "    signals = np.transpose(signals, (0, 2, 1))\n",
        "\n",
        "    # Reshape for CNN input (samples, channels, time_points, 1)\n",
        "    samples, time_points, channels = signals.shape\n",
        "    signals = signals.reshape(samples, time_points, channels, 1)\n",
        "\n",
        "    # Normalize\n",
        "    scaler = StandardScaler()\n",
        "    signals_scaled = scaler.fit_transform(signals.reshape(-1, channels)).reshape(signals.shape)\n",
        "\n",
        "    return signals_scaled\n",
        "\n",
        "X_scaled = preprocess_signals(X_raw)\n",
        "print(f\"X_scaled shape: {X_scaled.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b51fa31e-84e4-4e10-b5f7-5f744f11cf3c",
      "metadata": {
        "id": "b51fa31e-84e4-4e10-b5f7-5f744f11cf3c"
      },
      "source": [
        "## Step 2: Downsampling the Signals\n",
        "### Objective: Reduce the sampling rate of the EEG signals to make the data more manageable for processing.\n",
        "\n",
        "Explanation: EEG data often comes with a high sampling rate, such as 1000 Hz, which means there are 1000 data points per second. Downsampling reduces the number of data points, making it easier to handle computationally. Here, we downsample the signals from 1000 Hz to 250 Hz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b4ff6834-50d5-4206-b3ca-3129c035936d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4ff6834-50d5-4206-b3ca-3129c035936d",
        "outputId": "fa617bdb-4118-4cca-f675-07516d212055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_downsampled shape: (72, 21, 23500)\n"
          ]
        }
      ],
      "source": [
        "from scipy.signal import resample\n",
        "\n",
        "def downsample_signals(signals, target_length):\n",
        "    downsampled_signals = []\n",
        "    for signal in signals:\n",
        "        num_channels = signal.shape[0]\n",
        "        downsampled_signal = np.zeros((num_channels, target_length))\n",
        "        for i in range(num_channels):\n",
        "            downsampled_signal[i, :] = resample(signal[i, :], target_length)\n",
        "        downsampled_signals.append(downsampled_signal)\n",
        "    return np.array(downsampled_signals)\n",
        "\n",
        "# Assuming we want to downsample to 250 Hz from 1000 Hz and the original length is 94000 (for 60 seconds at 1000 Hz)\n",
        "original_length = 94000\n",
        "target_length = int(original_length * 250 / 1000)\n",
        "\n",
        "X_downsampled = downsample_signals(X_raw, target_length)\n",
        "print(f\"X_downsampled shape: {X_downsampled.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e814c922-1123-4a8f-be23-b2a9f704f78a",
      "metadata": {
        "id": "e814c922-1123-4a8f-be23-b2a9f704f78a"
      },
      "source": [
        "## Step 3: Segmenting the Signals\n",
        "### Objective: Split the continuous EEG signals into shorter, fixed-length segments.\n",
        "\n",
        "Explanation: Segmenting the EEG data into shorter chunks (e.g., 1-second segments) can improve the performance of machine learning models by providing more training samples. Overlapping segments can also be used to increase the dataset size further. In this step, we split the signals into 1-second segments with a 50% overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cb815d67-8b23-42b7-88b5-1906685852ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb815d67-8b23-42b7-88b5-1906685852ab",
        "outputId": "1bed4cf0-6523-4f1b-b2ea-d52ec23eeea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_segmented shape: (13464, 21, 250)\n"
          ]
        }
      ],
      "source": [
        "def segment_signals(signals, segment_length, overlap):\n",
        "    segments = []\n",
        "    for signal in signals:\n",
        "        num_channels, num_time_points = signal.shape\n",
        "        for start in range(0, num_time_points - segment_length + 1, segment_length // overlap):\n",
        "            segments.append(signal[:, start:start + segment_length])\n",
        "    return np.array(segments)\n",
        "\n",
        "segment_length = 250  # 1 second segments\n",
        "overlap = 2  # 50% overlap\n",
        "\n",
        "X_segmented = segment_signals(X_downsampled, segment_length, overlap)\n",
        "print(f\"X_segmented shape: {X_segmented.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "482d84aa-c24d-434e-a577-10eb1bbbcba8",
      "metadata": {
        "id": "482d84aa-c24d-434e-a577-10eb1bbbcba8"
      },
      "source": [
        "## Step 4: Preprocessing the Signals\n",
        "### Objective: Normalize and reshape the segmented signals to prepare them for input into the neural network.\n",
        "\n",
        "Explanation: Normalization ensures that the features have similar scales, which helps in faster convergence during training. The segmented signals are reshaped to match the input shape expected by the EEGNet model, which is (samples, time points, channels, 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c0d73fc7-dab7-43d6-99b5-010efa38eb59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0d73fc7-dab7-43d6-99b5-010efa38eb59",
        "outputId": "58d4633f-0e8d-4d74-992f-17944c41dfdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_preprocessed shape: (13464, 250, 21, 1)\n"
          ]
        }
      ],
      "source": [
        "def preprocess_signals(signals):\n",
        "    samples, channels, time_points = signals.shape\n",
        "    signals = signals.transpose((0, 2, 1)).reshape(samples, time_points, channels, 1)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    signals_scaled = scaler.fit_transform(signals.reshape(-1, channels)).reshape(signals.shape)\n",
        "\n",
        "    return signals_scaled\n",
        "\n",
        "X_preprocessed = preprocess_signals(X_segmented)\n",
        "print(f\"X_preprocessed shape: {X_preprocessed.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49291e09-cb63-47f3-9371-7b525fd52301",
      "metadata": {
        "id": "49291e09-cb63-47f3-9371-7b525fd52301"
      },
      "source": [
        "## Step 5: Creating Labels\n",
        "### Objective: Ensure that each segmented signal has a corresponding label.\n",
        "\n",
        "Explanation: Since we have segmented the data, we need to repeat the labels for each segment. This step ensures that each segment has a label corresponding to its original signal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cd150cbf-3190-474c-bcd9-c6eccf7dc2dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd150cbf-3190-474c-bcd9-c6eccf7dc2dd",
        "outputId": "b60931c7-8cad-45db-ff14-d46557ca9451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_repeated shape: (13464,)\n"
          ]
        }
      ],
      "source": [
        "# Repeat labels to match the number of segments per original sample\n",
        "num_segments_per_sample = X_segmented.shape[0] // len(X_downsampled)\n",
        "y_repeated = np.repeat(y_raw, num_segments_per_sample)\n",
        "print(f\"y_repeated shape: {y_repeated.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94b07c32-ac81-48cf-bcb1-4bcd3f2ea68c",
      "metadata": {
        "id": "94b07c32-ac81-48cf-bcb1-4bcd3f2ea68c"
      },
      "source": [
        "## Step 6: Splitting the Dataset\n",
        "### Objective: Split the dataset into training and testing sets.\n",
        "\n",
        "Explanation: To evaluate the performance of the model, we need to separate the data into training and testing sets. The training set is used to train the model, while the testing set is used to evaluate its performance on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ee8e9e63-b1b6-44ce-a307-25fb670f70f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee8e9e63-b1b6-44ce-a307-25fb670f70f2",
        "outputId": "2d07eb84-27f4-4c38-a8e5-c2fcff5db431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (10771, 250, 21, 1)\n",
            "X_test shape: (2693, 250, 21, 1)\n",
            "y_train shape: (10771,)\n",
            "y_test shape: (2693,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y_repeated, test_size=0.2, random_state=42)\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3b7165c0-43a6-46e0-9ee4-6b77156aae17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b7165c0-43a6-46e0-9ee4-6b77156aae17",
        "outputId": "7440b6c0-8ee6-46d6-a254-946111299873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_patches shape: (10771, 210, 25)\n",
            "X_test_patches shape: (2693, 210, 25)\n"
          ]
        }
      ],
      "source": [
        "def create_patches(X, patch_size):\n",
        "    patches = []\n",
        "    for signal in X:\n",
        "        signal_patches = np.array([signal[i:i + patch_size] for i in range(0, signal.shape[0] - patch_size + 1, patch_size)])\n",
        "        patches.append(signal_patches)\n",
        "    return np.array(patches)\n",
        "\n",
        "patch_size = 25  # Adjust this based on your data and ViT configuration\n",
        "X_patches = create_patches(X_train, patch_size)\n",
        "X_patches = X_patches.reshape(X_patches.shape[0], -1, patch_size * X_patches.shape[-1])\n",
        "print(f\"X_patches shape: {X_patches.shape}\")\n",
        "\n",
        "# Prepare the test data similarly\n",
        "X_test_patches = create_patches(X_test, patch_size)\n",
        "X_test_patches = X_test_patches.reshape(X_test_patches.shape[0], -1, patch_size * X_test_patches.shape[-1])\n",
        "print(f\"X_test_patches shape: {X_test_patches.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8962e0d4-2afa-4919-af87-551ead30af20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8962e0d4-2afa-4919-af87-551ead30af20",
        "outputId": "86416d5f-eb7c-4c5a-fbd1-871299bb30ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 210, 25)]            0         []                            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 210, 64)              1664      ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 210, 64)              128       ['dense[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  (None, 210, 64)              66368     ['layer_normalization[0][0]', \n",
            " iHeadAttention)                                                     'layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 210, 64)              0         ['multi_head_attention[0][0]']\n",
            "                                                                                                  \n",
            " add (Add)                   (None, 210, 64)              0         ['layer_normalization[0][0]', \n",
            "                                                                     'dropout[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 210, 64)              128       ['add[0][0]']                 \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " sequential (Sequential)     (None, 210, 64)              8320      ['layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 210, 64)              0         ['sequential[0][0]']          \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 210, 64)              0         ['layer_normalization_1[0][0]'\n",
            "                                                                    , 'dropout_1[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, 210, 64)              128       ['add_1[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (Mu  (None, 210, 64)              66368     ['layer_normalization_2[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 210, 64)              0         ['multi_head_attention_1[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 210, 64)              0         ['layer_normalization_2[0][0]'\n",
            "                                                                    , 'dropout_2[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_3 (Lay  (None, 210, 64)              128       ['add_2[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " sequential_1 (Sequential)   (None, 210, 64)              8320      ['layer_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 210, 64)              0         ['sequential_1[0][0]']        \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, 210, 64)              0         ['layer_normalization_3[0][0]'\n",
            "                                                                    , 'dropout_3[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, 210, 64)              128       ['add_3[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " global_average_pooling1d (  (None, 64)                   0         ['layer_normalization_4[0][0]'\n",
            " GlobalAveragePooling1D)                                            ]                             \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 2)                    130       ['global_average_pooling1d[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 151810 (593.01 KB)\n",
            "Trainable params: 151810 (593.01 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LayerNormalization, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def ViT(patch_size, num_patches, num_classes, d_model, num_heads, num_layers, mlp_dim, dropout_rate):\n",
        "    inputs = Input(shape=(num_patches, patch_size))\n",
        "    x = Dense(d_model)(inputs)\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        # Multi-head Self-Attention\n",
        "        attn_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
        "        attn_output = Dropout(dropout_rate)(attn_output)\n",
        "        x = tf.keras.layers.Add()([x, attn_output])\n",
        "        x = LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        # Feed-Forward Network\n",
        "        ffn = tf.keras.Sequential([\n",
        "            Dense(mlp_dim, activation='relu'),\n",
        "            Dense(d_model),\n",
        "        ])\n",
        "        ffn_output = ffn(x)\n",
        "        ffn_output = Dropout(dropout_rate)(ffn_output)\n",
        "        x = tf.keras.layers.Add()([x, ffn_output])\n",
        "        x = LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "# Parameters\n",
        "num_classes = len(np.unique(y_train))\n",
        "num_patches = X_patches.shape[1]\n",
        "d_model = 64  # Embedding dimension\n",
        "num_heads = 4  # Number of attention heads\n",
        "num_layers = 2  # Number of transformer layers\n",
        "mlp_dim = 64  # Dimension of the feed-forward layer\n",
        "dropout_rate = 0.1  # Dropout rate\n",
        "\n",
        "# Create the model\n",
        "vit_model = ViT(patch_size, num_patches, num_classes, d_model, num_heads, num_layers, mlp_dim, dropout_rate)\n",
        "\n",
        "# Compile the model\n",
        "vit_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "vit_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d86bd6f0-ac67-49ab-90d5-f6a9faa46b59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d86bd6f0-ac67-49ab-90d5-f6a9faa46b59",
        "outputId": "a9a0fdec-6c22-427e-b7f6-89d4c39c64ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "337/337 [==============================] - 79s 224ms/step - loss: 0.6049 - accuracy: 0.7206 - val_loss: 0.5942 - val_accuracy: 0.7182\n",
            "Epoch 2/100\n",
            "337/337 [==============================] - 74s 219ms/step - loss: 0.5941 - accuracy: 0.7232 - val_loss: 0.5955 - val_accuracy: 0.7182\n",
            "Epoch 3/100\n",
            "337/337 [==============================] - 72s 214ms/step - loss: 0.5925 - accuracy: 0.7232 - val_loss: 0.5921 - val_accuracy: 0.7182\n",
            "Epoch 4/100\n",
            "337/337 [==============================] - 72s 213ms/step - loss: 0.5904 - accuracy: 0.7237 - val_loss: 0.5938 - val_accuracy: 0.7182\n",
            "Epoch 5/100\n",
            "337/337 [==============================] - 72s 213ms/step - loss: 0.5848 - accuracy: 0.7228 - val_loss: 0.5875 - val_accuracy: 0.7185\n",
            "Epoch 6/100\n",
            "337/337 [==============================] - 71s 212ms/step - loss: 0.5758 - accuracy: 0.7239 - val_loss: 0.5823 - val_accuracy: 0.7282\n",
            "Epoch 7/100\n",
            "337/337 [==============================] - 70s 208ms/step - loss: 0.5629 - accuracy: 0.7293 - val_loss: 0.5604 - val_accuracy: 0.7401\n",
            "Epoch 8/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.5459 - accuracy: 0.7403 - val_loss: 0.5621 - val_accuracy: 0.7319\n",
            "Epoch 9/100\n",
            "337/337 [==============================] - 71s 209ms/step - loss: 0.5213 - accuracy: 0.7541 - val_loss: 0.5292 - val_accuracy: 0.7519\n",
            "Epoch 10/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.4939 - accuracy: 0.7714 - val_loss: 0.5176 - val_accuracy: 0.7646\n",
            "Epoch 11/100\n",
            "337/337 [==============================] - 71s 209ms/step - loss: 0.4729 - accuracy: 0.7846 - val_loss: 0.5181 - val_accuracy: 0.7679\n",
            "Epoch 12/100\n",
            "337/337 [==============================] - 70s 208ms/step - loss: 0.4496 - accuracy: 0.7967 - val_loss: 0.5586 - val_accuracy: 0.7334\n",
            "Epoch 13/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.4335 - accuracy: 0.8061 - val_loss: 0.5131 - val_accuracy: 0.7616\n",
            "Epoch 14/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.4121 - accuracy: 0.8166 - val_loss: 0.4974 - val_accuracy: 0.7750\n",
            "Epoch 15/100\n",
            "337/337 [==============================] - 70s 206ms/step - loss: 0.4044 - accuracy: 0.8209 - val_loss: 0.4947 - val_accuracy: 0.7776\n",
            "Epoch 16/100\n",
            "337/337 [==============================] - 69s 206ms/step - loss: 0.3878 - accuracy: 0.8293 - val_loss: 0.4875 - val_accuracy: 0.7883\n",
            "Epoch 17/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.3702 - accuracy: 0.8366 - val_loss: 0.4887 - val_accuracy: 0.7820\n",
            "Epoch 18/100\n",
            "337/337 [==============================] - 68s 203ms/step - loss: 0.3632 - accuracy: 0.8407 - val_loss: 0.5055 - val_accuracy: 0.7831\n",
            "Epoch 19/100\n",
            "337/337 [==============================] - 70s 208ms/step - loss: 0.3412 - accuracy: 0.8493 - val_loss: 0.5010 - val_accuracy: 0.7802\n",
            "Epoch 20/100\n",
            "337/337 [==============================] - 71s 210ms/step - loss: 0.3480 - accuracy: 0.8463 - val_loss: 0.5166 - val_accuracy: 0.7798\n",
            "Epoch 21/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.3378 - accuracy: 0.8531 - val_loss: 0.5449 - val_accuracy: 0.7694\n",
            "Epoch 22/100\n",
            "337/337 [==============================] - 70s 209ms/step - loss: 0.3315 - accuracy: 0.8530 - val_loss: 0.5398 - val_accuracy: 0.7839\n",
            "Epoch 23/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.3249 - accuracy: 0.8553 - val_loss: 0.5285 - val_accuracy: 0.7895\n",
            "Epoch 24/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.3178 - accuracy: 0.8589 - val_loss: 0.5389 - val_accuracy: 0.7869\n",
            "Epoch 25/100\n",
            "337/337 [==============================] - 69s 206ms/step - loss: 0.3121 - accuracy: 0.8618 - val_loss: 0.5523 - val_accuracy: 0.7932\n",
            "Epoch 26/100\n",
            "337/337 [==============================] - 69s 205ms/step - loss: 0.3102 - accuracy: 0.8602 - val_loss: 0.5348 - val_accuracy: 0.7909\n",
            "Epoch 27/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.3140 - accuracy: 0.8610 - val_loss: 0.5028 - val_accuracy: 0.7954\n",
            "Epoch 28/100\n",
            "337/337 [==============================] - 70s 208ms/step - loss: 0.3008 - accuracy: 0.8662 - val_loss: 0.5508 - val_accuracy: 0.7824\n",
            "Epoch 29/100\n",
            "337/337 [==============================] - 69s 206ms/step - loss: 0.2996 - accuracy: 0.8671 - val_loss: 0.5323 - val_accuracy: 0.7939\n",
            "Epoch 30/100\n",
            "337/337 [==============================] - 69s 206ms/step - loss: 0.2962 - accuracy: 0.8683 - val_loss: 0.5399 - val_accuracy: 0.7965\n",
            "Epoch 31/100\n",
            "337/337 [==============================] - 69s 205ms/step - loss: 0.2969 - accuracy: 0.8664 - val_loss: 0.5358 - val_accuracy: 0.7898\n",
            "Epoch 32/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2914 - accuracy: 0.8697 - val_loss: 0.5504 - val_accuracy: 0.7876\n",
            "Epoch 33/100\n",
            "337/337 [==============================] - 70s 208ms/step - loss: 0.2930 - accuracy: 0.8698 - val_loss: 0.5807 - val_accuracy: 0.7987\n",
            "Epoch 34/100\n",
            "337/337 [==============================] - 69s 206ms/step - loss: 0.2922 - accuracy: 0.8709 - val_loss: 0.5596 - val_accuracy: 0.7939\n",
            "Epoch 35/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2892 - accuracy: 0.8712 - val_loss: 0.5476 - val_accuracy: 0.7984\n",
            "Epoch 36/100\n",
            "337/337 [==============================] - 69s 205ms/step - loss: 0.2886 - accuracy: 0.8696 - val_loss: 0.5603 - val_accuracy: 0.7906\n",
            "Epoch 37/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2856 - accuracy: 0.8733 - val_loss: 0.5461 - val_accuracy: 0.7928\n",
            "Epoch 38/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2835 - accuracy: 0.8747 - val_loss: 0.5415 - val_accuracy: 0.8054\n",
            "Epoch 39/100\n",
            "337/337 [==============================] - 70s 208ms/step - loss: 0.2883 - accuracy: 0.8709 - val_loss: 0.5751 - val_accuracy: 0.7835\n",
            "Epoch 40/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2803 - accuracy: 0.8748 - val_loss: 0.5878 - val_accuracy: 0.7991\n",
            "Epoch 41/100\n",
            "337/337 [==============================] - 70s 208ms/step - loss: 0.2826 - accuracy: 0.8724 - val_loss: 0.5273 - val_accuracy: 0.7947\n",
            "Epoch 42/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2801 - accuracy: 0.8737 - val_loss: 0.5807 - val_accuracy: 0.8051\n",
            "Epoch 43/100\n",
            "337/337 [==============================] - 70s 206ms/step - loss: 0.2729 - accuracy: 0.8782 - val_loss: 0.5836 - val_accuracy: 0.8006\n",
            "Epoch 44/100\n",
            "337/337 [==============================] - 70s 208ms/step - loss: 0.2728 - accuracy: 0.8785 - val_loss: 0.5674 - val_accuracy: 0.7980\n",
            "Epoch 45/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2702 - accuracy: 0.8793 - val_loss: 0.5612 - val_accuracy: 0.8073\n",
            "Epoch 46/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2789 - accuracy: 0.8742 - val_loss: 0.5391 - val_accuracy: 0.7991\n",
            "Epoch 47/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2704 - accuracy: 0.8788 - val_loss: 0.6171 - val_accuracy: 0.7661\n",
            "Epoch 48/100\n",
            "337/337 [==============================] - 70s 206ms/step - loss: 0.2720 - accuracy: 0.8791 - val_loss: 0.5696 - val_accuracy: 0.8021\n",
            "Epoch 49/100\n",
            "337/337 [==============================] - 69s 205ms/step - loss: 0.2747 - accuracy: 0.8775 - val_loss: 0.5547 - val_accuracy: 0.7883\n",
            "Epoch 50/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2794 - accuracy: 0.8752 - val_loss: 0.5582 - val_accuracy: 0.7987\n",
            "Epoch 51/100\n",
            "337/337 [==============================] - 70s 209ms/step - loss: 0.2748 - accuracy: 0.8766 - val_loss: 0.6606 - val_accuracy: 0.7947\n",
            "Epoch 52/100\n",
            "337/337 [==============================] - 70s 209ms/step - loss: 0.2663 - accuracy: 0.8796 - val_loss: 0.6833 - val_accuracy: 0.8006\n",
            "Epoch 53/100\n",
            "337/337 [==============================] - 70s 208ms/step - loss: 0.2701 - accuracy: 0.8790 - val_loss: 0.6231 - val_accuracy: 0.7980\n",
            "Epoch 54/100\n",
            "337/337 [==============================] - 69s 205ms/step - loss: 0.2684 - accuracy: 0.8793 - val_loss: 0.5431 - val_accuracy: 0.7995\n",
            "Epoch 55/100\n",
            "337/337 [==============================] - 70s 206ms/step - loss: 0.2684 - accuracy: 0.8804 - val_loss: 0.5853 - val_accuracy: 0.8025\n",
            "Epoch 56/100\n",
            "337/337 [==============================] - 69s 204ms/step - loss: 0.2633 - accuracy: 0.8811 - val_loss: 0.5886 - val_accuracy: 0.8021\n",
            "Epoch 57/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2659 - accuracy: 0.8809 - val_loss: 0.5599 - val_accuracy: 0.8058\n",
            "Epoch 58/100\n",
            "337/337 [==============================] - 69s 206ms/step - loss: 0.2617 - accuracy: 0.8817 - val_loss: 0.5748 - val_accuracy: 0.8013\n",
            "Epoch 59/100\n",
            "337/337 [==============================] - 69s 205ms/step - loss: 0.2561 - accuracy: 0.8852 - val_loss: 0.6226 - val_accuracy: 0.8065\n",
            "Epoch 60/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2664 - accuracy: 0.8824 - val_loss: 0.6198 - val_accuracy: 0.8088\n",
            "Epoch 61/100\n",
            "337/337 [==============================] - 69s 205ms/step - loss: 0.2671 - accuracy: 0.8802 - val_loss: 0.5637 - val_accuracy: 0.8006\n",
            "Epoch 62/100\n",
            "337/337 [==============================] - 68s 201ms/step - loss: 0.2641 - accuracy: 0.8802 - val_loss: 0.5628 - val_accuracy: 0.8110\n",
            "Epoch 63/100\n",
            "337/337 [==============================] - 69s 206ms/step - loss: 0.2617 - accuracy: 0.8821 - val_loss: 0.5728 - val_accuracy: 0.8106\n",
            "Epoch 64/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2608 - accuracy: 0.8834 - val_loss: 0.5830 - val_accuracy: 0.8080\n",
            "Epoch 65/100\n",
            "337/337 [==============================] - 69s 205ms/step - loss: 0.2652 - accuracy: 0.8804 - val_loss: 0.5855 - val_accuracy: 0.8062\n",
            "Epoch 66/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2576 - accuracy: 0.8829 - val_loss: 0.5615 - val_accuracy: 0.7965\n",
            "Epoch 67/100\n",
            "337/337 [==============================] - 69s 206ms/step - loss: 0.2652 - accuracy: 0.8809 - val_loss: 0.5802 - val_accuracy: 0.7924\n",
            "Epoch 68/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2694 - accuracy: 0.8791 - val_loss: 0.4832 - val_accuracy: 0.8128\n",
            "Epoch 69/100\n",
            "337/337 [==============================] - 70s 208ms/step - loss: 0.2479 - accuracy: 0.8880 - val_loss: 0.5544 - val_accuracy: 0.8140\n",
            "Epoch 70/100\n",
            "337/337 [==============================] - 70s 208ms/step - loss: 0.2611 - accuracy: 0.8808 - val_loss: 0.5690 - val_accuracy: 0.8043\n",
            "Epoch 71/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2607 - accuracy: 0.8827 - val_loss: 0.5798 - val_accuracy: 0.8051\n",
            "Epoch 72/100\n",
            "337/337 [==============================] - 69s 206ms/step - loss: 0.2617 - accuracy: 0.8824 - val_loss: 0.5743 - val_accuracy: 0.8106\n",
            "Epoch 73/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2557 - accuracy: 0.8856 - val_loss: 0.5629 - val_accuracy: 0.8058\n",
            "Epoch 74/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2571 - accuracy: 0.8847 - val_loss: 0.5864 - val_accuracy: 0.8025\n",
            "Epoch 75/100\n",
            "337/337 [==============================] - 70s 208ms/step - loss: 0.2696 - accuracy: 0.8782 - val_loss: 0.5451 - val_accuracy: 0.8091\n",
            "Epoch 76/100\n",
            "337/337 [==============================] - 69s 206ms/step - loss: 0.2471 - accuracy: 0.8883 - val_loss: 0.6038 - val_accuracy: 0.8073\n",
            "Epoch 77/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2638 - accuracy: 0.8824 - val_loss: 0.5456 - val_accuracy: 0.8136\n",
            "Epoch 78/100\n",
            "337/337 [==============================] - 69s 205ms/step - loss: 0.2583 - accuracy: 0.8841 - val_loss: 0.5567 - val_accuracy: 0.8117\n",
            "Epoch 79/100\n",
            "337/337 [==============================] - 70s 206ms/step - loss: 0.2551 - accuracy: 0.8852 - val_loss: 0.5833 - val_accuracy: 0.8062\n",
            "Epoch 80/100\n",
            "337/337 [==============================] - 69s 206ms/step - loss: 0.2566 - accuracy: 0.8849 - val_loss: 0.5935 - val_accuracy: 0.8028\n",
            "Epoch 81/100\n",
            "337/337 [==============================] - 69s 206ms/step - loss: 0.2617 - accuracy: 0.8832 - val_loss: 0.5719 - val_accuracy: 0.8143\n",
            "Epoch 82/100\n",
            "337/337 [==============================] - 69s 206ms/step - loss: 0.2508 - accuracy: 0.8859 - val_loss: 0.6445 - val_accuracy: 0.7995\n",
            "Epoch 83/100\n",
            "337/337 [==============================] - 69s 205ms/step - loss: 0.2543 - accuracy: 0.8860 - val_loss: 0.6071 - val_accuracy: 0.8051\n",
            "Epoch 84/100\n",
            "337/337 [==============================] - 69s 205ms/step - loss: 0.2553 - accuracy: 0.8839 - val_loss: 0.5546 - val_accuracy: 0.8147\n",
            "Epoch 85/100\n",
            "337/337 [==============================] - 69s 205ms/step - loss: 0.2523 - accuracy: 0.8861 - val_loss: 0.5466 - val_accuracy: 0.8166\n",
            "Epoch 86/100\n",
            "337/337 [==============================] - 70s 209ms/step - loss: 0.2549 - accuracy: 0.8850 - val_loss: 0.5700 - val_accuracy: 0.8088\n",
            "Epoch 87/100\n",
            "337/337 [==============================] - 70s 206ms/step - loss: 0.2529 - accuracy: 0.8844 - val_loss: 0.6182 - val_accuracy: 0.8102\n",
            "Epoch 88/100\n",
            "337/337 [==============================] - 69s 205ms/step - loss: 0.2586 - accuracy: 0.8838 - val_loss: 0.5723 - val_accuracy: 0.8076\n",
            "Epoch 89/100\n",
            "337/337 [==============================] - 68s 203ms/step - loss: 0.2588 - accuracy: 0.8834 - val_loss: 0.5911 - val_accuracy: 0.8125\n",
            "Epoch 90/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2488 - accuracy: 0.8879 - val_loss: 0.5990 - val_accuracy: 0.8154\n",
            "Epoch 91/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2550 - accuracy: 0.8842 - val_loss: 0.7010 - val_accuracy: 0.8013\n",
            "Epoch 92/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2594 - accuracy: 0.8823 - val_loss: 0.5611 - val_accuracy: 0.8080\n",
            "Epoch 93/100\n",
            "337/337 [==============================] - 69s 206ms/step - loss: 0.2561 - accuracy: 0.8856 - val_loss: 0.5631 - val_accuracy: 0.8080\n",
            "Epoch 94/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2504 - accuracy: 0.8877 - val_loss: 0.5613 - val_accuracy: 0.8028\n",
            "Epoch 95/100\n",
            "337/337 [==============================] - 70s 209ms/step - loss: 0.2527 - accuracy: 0.8859 - val_loss: 0.6016 - val_accuracy: 0.8177\n",
            "Epoch 96/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2520 - accuracy: 0.8854 - val_loss: 0.6227 - val_accuracy: 0.8143\n",
            "Epoch 97/100\n",
            "337/337 [==============================] - 70s 208ms/step - loss: 0.2445 - accuracy: 0.8895 - val_loss: 0.6079 - val_accuracy: 0.8136\n",
            "Epoch 98/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2669 - accuracy: 0.8803 - val_loss: 0.5688 - val_accuracy: 0.8076\n",
            "Epoch 99/100\n",
            "337/337 [==============================] - 69s 206ms/step - loss: 0.2464 - accuracy: 0.8874 - val_loss: 0.5642 - val_accuracy: 0.8117\n",
            "Epoch 100/100\n",
            "337/337 [==============================] - 70s 207ms/step - loss: 0.2644 - accuracy: 0.8826 - val_loss: 0.6280 - val_accuracy: 0.7802\n",
            "Test loss: 0.6280481219291687\n",
            "Test accuracy: 0.7801707983016968\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = vit_model.fit(X_patches, y_train, epochs=100, batch_size=32, validation_data=(X_test_patches, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "score = vit_model.evaluate(X_test_patches, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}